{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Trump/Hillary Tweets\n",
    "### Question: \n",
    "\n",
    "What combination of words result in the highest amount of retweets? As in, are there a set of #N words that Trump or Hillary could tweet that garners the most retweets and likes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Trump and Hillary Tweets before the 2016 Presidential Election and analyzing these two datasets to find the most common words that each respective candidates used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Basic packages to be used in the project. \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import string\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#NLTK tokenizer for tweets.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning the Data\n",
    "\n",
    "We are loading both the datasets so we can retrieve both Trump and Hillary tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dataframe of Trump tweets.\n",
    "df_trump = pd.read_csv('Trump_Tweets.csv', encoding='latin-1');\n",
    "\n",
    "#Dataframe of Hillary tweets.\n",
    "df_th = pd.read_csv('Trump_Hillary_Tweets.csv');\n",
    "df_hillary = df_th[df_th['handle'] == 'HillaryClinton'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cleaned unnecessary columns of the Trump tweets.\n",
    "del df_trump['Unnamed: 10'];\n",
    "del df_trump['Unnamed: 11'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TESTING\n",
    "#df_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TESTING\n",
    "#tweet = tknzr.tokenize(df_trump['Tweet_Text'][0])\n",
    "#tweet = words_stop(tweet)\n",
    "#tweet = words_only(tweet)\n",
    "#tweet = words_extra(tweet)\n",
    "#print(tweet);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Helper method to filter out stopwords.\n",
    "def words_stop(tweet_list):\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop = stopwords.words('english') + punctuation + ['rt','via']\n",
    "    return [word for word in tweet_list if word not in stop]\n",
    "\n",
    "#Helper method to filter out hashtags and mentions.\n",
    "def words_only(tweet_list):\n",
    "    return [word for word in tweet_list if not word.startswith(('#','@','û','https'))]\n",
    "\n",
    "#Helper method to filter extra words.\n",
    "def words_extra(tweet_list):\n",
    "    extra = ['\\x89','...','…','“','”','’','—']\n",
    "    return [word for word in tweet_list if word not in extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parsing the Trump Tweets.\n",
    "\n",
    "We are parsing the Trump tweets, so we can create a frequency distribution of words contained in his tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#METHOD: Parse Trump tweets and create a frequency distribution of words.\n",
    "\n",
    "#Tokenizes the Trump tweets.\n",
    "trump_list = []\n",
    "for trump_tweets in df_trump['Tweet_Text']:\n",
    "    trump_list.extend(tknzr.tokenize(trump_tweets))\n",
    "\n",
    "#Filters the tweets.\n",
    "trump_list = words_stop(trump_list)\n",
    "trump_list = words_only(trump_list)\n",
    "trump_list = words_extra(trump_list)\n",
    "\n",
    "#Create the frequency distribution.\n",
    "fdist_t = nltk.FreqDist(trump_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(fdist_t);\n",
    "#trump_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_t.most_common(20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing the Hillary Tweets.\n",
    "\n",
    "We are parsing the Hillary tweets, so we can create a frequency distribution of words contained in her tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Tokenizes the Hillary tweets.\n",
    "hillary_list = []\n",
    "for hillary_tweets in df_hillary['text']:\n",
    "    hillary_list.extend(tknzr.tokenize(hillary_tweets))\n",
    "\n",
    "#Filters the tweets.\n",
    "hillary_list = words_stop(hillary_list)\n",
    "hillary_list = words_only(hillary_list)\n",
    "hillary_list = words_extra(hillary_list)\n",
    "\n",
    "#Create the frequency distribution.\n",
    "fdist_h = nltk.FreqDist(hillary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(fdist);\n",
    "#hillary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_h.most_common(20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Dictionary of words with favourites and retweets.\n",
    "\n",
    "Create a dictionary with the words as the key and a tuple of retweets and favorites, unweighted. Then, weigh retweets more heavily by multiplying by the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Trump tweets dictionary.\n",
    "from collections import namedtuple\n",
    "Tweets = namedtuple('Tweets', 'favourites retweets')\n",
    "\n",
    "trump_dict = {}\n",
    "i = 0\n",
    "\n",
    "for trump_tweet in df_trump['Tweet_Text']:\n",
    "    \n",
    "    tweet = tknzr.tokenize(trump_tweet)\n",
    "    tweet = words_stop(tweet)\n",
    "    tweet = words_only(tweet)\n",
    "    tweet = words_extra(tweet)\n",
    "    \n",
    "    for word in tweet:\n",
    "        trump_dict.setdefault(word, Tweets(0, 0))\n",
    "        num_fav = df_trump['twt_favourites_IS_THIS_LIKE_QUESTION_MARK'][i]\n",
    "        num_rtwt = df_trump['Retweets'][i]\n",
    "        \n",
    "        fav = trump_dict[word].favourites + num_fav\n",
    "        rtwt = trump_dict[word].retweets + num_rtwt\n",
    "        \n",
    "        trump_dict[word] = trump_dict[word]._replace(favourites = fav, retweets = rtwt)\n",
    "        \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_dict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hillary tweets dictionary.\n",
    "from collections import namedtuple\n",
    "Tweets = namedtuple('Tweets', 'favourites retweets')\n",
    "\n",
    "hillary_dict = {}\n",
    "i = 0\n",
    "\n",
    "for hillary_tweet in df_hillary['text']:\n",
    "    \n",
    "    tweet = tknzr.tokenize(hillary_tweet)\n",
    "    tweet = words_stop(tweet)\n",
    "    tweet = words_only(tweet)\n",
    "    tweet = words_extra(tweet)\n",
    "    \n",
    "    for word in tweet:\n",
    "        hillary_dict.setdefault(word, Tweets(0, 0))\n",
    "        num_fav = df_hillary['favorite_count'].iloc[i]\n",
    "        num_rtwt = df_hillary['retweet_count'].iloc[i]\n",
    "        \n",
    "        fav = hillary_dict[word].favourites + num_fav\n",
    "        rtwt = hillary_dict[word].retweets + num_rtwt\n",
    "        \n",
    "        hillary_dict[word] = hillary_dict[word]._replace(favourites = fav, retweets = rtwt)\n",
    "        \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hillary_dict;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the ratio between favorites and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of favorites to retweet is:  2.5952112676056336\n"
     ]
    }
   ],
   "source": [
    "#METHOD: We are finding the ratio between favorites and retweets. \n",
    "\n",
    "#Weigh retweets more heavily over favorites. Multiply retweets by ratio so it is weighed.\n",
    "\n",
    "sum_fav = df_trump['twt_favourites_IS_THIS_LIKE_QUESTION_MARK'].sum();\n",
    "sum_rtwt = df_trump['Retweets'].sum();\n",
    "\n",
    "ratio = sum_fav/sum_rtwt\n",
    "\n",
    "print('The ratio of favorites to retweet is: ', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a histogram. (1-2 graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
