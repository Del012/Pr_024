{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are using Trump and Hillary Tweets before the 2016 Presidential Election and analyzing these two datasets to find the most common words that each respective candidates used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Packages to be used in the project. \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download('all')\n",
    "#nltk.download('corpus')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dataframes for Trump/Hillary tweets.\n",
    "df_trump = pd.read_csv('Trump_Tweets.csv', encoding='latin-1');\n",
    "\n",
    "df_trump_hillary = pd.read_csv('Trump_Hillary_Tweets.csv');\n",
    "df_hillary = df_trump_hillary[df_trump_hillary['handle'] == 'HillaryClinton'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cleaned the Trump tweets.\n",
    "del df_trump['Unnamed: 10'];\n",
    "del df_trump['Unnamed: 11'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hillary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'we', 'express', 'our', 'deepest', 'gratitude', 'to', 'all', 'those', 'who', 'have', 'served', 'in', 'our', 'armed', 'forces.', '#ThankAVet', 'https://t.co/wPk7QWpK8Z']\n"
     ]
    }
   ],
   "source": [
    "tweet = df_trump['Tweet_Text'][0].split();\n",
    "print(tweet);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Trump Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#METHOD: Parse Trump tweets and tokenize them for frequencies.\n",
    "\n",
    "#Retrieved all the trump tweets and combined it into one big string.\n",
    "whole_str = '';\n",
    "\n",
    "for trump_tweets in df_trump['Tweet_Text']:\n",
    "    whole_str += str(' ' + trump_tweets)\n",
    "    \n",
    "#Tokenized the big Trump string.\n",
    "tokens = nltk.tokenize.word_tokenize(whole_str); \n",
    "\n",
    "#Filter out all punctuation and common words.\n",
    "from nltk.corpus import stopwords\n",
    "s = set(stopwords.words('english'))\n",
    "f1 = filter(lambda w: not w in s,tokens)\n",
    "f2 = filter(lambda w: not w in string.punctuation, f1)\n",
    "f3 = filter(lambda w: not w in ('``','...','--',\"''\",'https','http','I','amp','The'), f2)\n",
    "\n",
    "fdist = nltk.FreqDist(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 17658 samples and 83048 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('realDonaldTrump', 1507),\n",
       " ('Trump', 1003),\n",
       " ('Thank', 753),\n",
       " ('Trump2016', 596),\n",
       " ('great', 555),\n",
       " ('Hillary', 528),\n",
       " ('MakeAmericaGreatAgain', 508),\n",
       " ('RT', 448),\n",
       " ('people', 376),\n",
       " ('We', 344),\n",
       " ('Donald', 304),\n",
       " ('Great', 299),\n",
       " ('CNN', 298),\n",
       " ('Clinton', 294),\n",
       " ('FoxNews', 272),\n",
       " ('America', 269),\n",
       " ('like', 253),\n",
       " ('New', 252),\n",
       " ('get', 230),\n",
       " ('Will', 225),\n",
       " ('tonight', 222),\n",
       " ('Crooked', 215),\n",
       " ('AMERICA', 212),\n",
       " ('Cruz', 212),\n",
       " ('poll', 208),\n",
       " ('going', 206),\n",
       " ('A', 203),\n",
       " ('debate', 194),\n",
       " ('one', 193),\n",
       " ('GREAT', 191),\n",
       " ('would', 191),\n",
       " ('last', 188),\n",
       " ('country', 181),\n",
       " ('said', 180),\n",
       " ('time', 175),\n",
       " ('back', 174),\n",
       " ('big', 174),\n",
       " ('He', 174),\n",
       " ('Just', 168),\n",
       " ('You', 168),\n",
       " ('MAKE', 165),\n",
       " ('Iowa', 164),\n",
       " ('AGAIN', 162),\n",
       " ('much', 161),\n",
       " ('want', 157),\n",
       " ('vote', 156),\n",
       " ('President', 155),\n",
       " ('GOP', 155),\n",
       " ('TRUMP', 151),\n",
       " ('many', 150)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Hillary Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#METHOD: Parse Hillary Tweets and tokenize them for frequencies.\n",
    "\n",
    "#Retrieved all the Hillary Tweets and combined it into one big string.\n",
    "hillary_str = '';\n",
    "\n",
    "for hillary_tweets in df_hillary['text']:\n",
    "    hillary_str += str(' ' + hillary_tweets)\n",
    "    \n",
    "#Tokenized the big Hillary string.\n",
    "hillary_tokens = nltk.tokenize.word_tokenize(hillary_str); \n",
    "\n",
    "#Filter out all punctuation and common words.\n",
    "from nltk.corpus import stopwords\n",
    "s_h = set(stopwords.words('english'))\n",
    "f1_h = filter(lambda w: not w in s_h, hillary_tokens)\n",
    "f2_h = filter(lambda w: not w in string.punctuation, f1_h)\n",
    "f3_h = filter(lambda w: not w in ('``','...','--',\"''\",'https','http','I','amp'), f2_h)\n",
    "\n",
    "fdist_h = nltk.FreqDist(f3_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Trump', 865),\n",
       " (\"'s\", 807),\n",
       " ('Hillary', 424),\n",
       " ('Donald', 400),\n",
       " ('We', 366),\n",
       " ('—', 347),\n",
       " ('—Hillary', 308),\n",
       " (\"n't\", 237),\n",
       " ('president', 215),\n",
       " ('America', 190),\n",
       " ('people', 182),\n",
       " ('make', 170),\n",
       " ('The', 165),\n",
       " ('us', 162),\n",
       " ('one', 148),\n",
       " ('POTUS', 144),\n",
       " ('families', 131),\n",
       " ('need', 129),\n",
       " ('Americans', 125),\n",
       " ('would', 122)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_h.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the ratio between favorites and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of favorites to retweet is:  2.5952112676056336\n"
     ]
    }
   ],
   "source": [
    "#METHOD: We are finding the ratio between favorites and retweets. \n",
    "\n",
    "#Weigh retweets more heavily over favorites. Multiply retweets by ratio so it is weighed.\n",
    "\n",
    "sum_fav = df_trump['twt_favourites_IS_THIS_LIKE_QUESTION_MARK'].sum();\n",
    "sum_rtwt = df_trump['Retweets'].sum();\n",
    "\n",
    "ratio = sum_fav/sum_rtwt\n",
    "\n",
    "print('The ratio of favorites to retweet is: ', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create tuple of retweets/favorites in relation to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a dictionary with words as the key and a tuple of retweets and favorites, unweighted.\n",
    "#Then, weigh retweets more heavily by multiplying by the ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a histogram. (1-2 graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
